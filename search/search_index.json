{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the documentation webpage of the LSSCB cluster. Section Description Getting started Basic information about the account setup and connections and data transfer. Running calculations Information on jobs scheduling, SLURM and best practices of interactive work. FAQ Frequently asked questions and solutions to common problems. Available resources Technical description of available resources. Available software List of preinstalled software packages available to all users.","title":"Home"},{"location":"faq/","text":"How to run Jupyter notebooks remotely on LSSCB? Assuming jupyter is running on cn000 (port 8888 ) and that the connection is established with sshuttle (see Getting started section for more details) it is possible setup the tunnel as follows: ssh -NL 8888 :localhost:8888 your_username@cn000.sih-59.internal Afterwards you should be able to see the running Jupyter instance via browser at the URL: http://localhost:8888 How to install python packages on LSSCB? python3 (3.10) as well as python2 (2.7.18) along with the recent pip and venv are installed system-wide on each node of LSSCB. You can simply install packages either with pip install --user or use venv or virtualenv (this will allow to handle multiple projects with possibly conflicting dependencies). Finally, if you need newer versions of python or want to handle complicated dependencies you can install a local version of anaconda in your $HOME directory. I want to use program XXX on LSSCB, can I install it on my own? You can install any software you like in your $HOME directory (as long as you have a valid license to use it). If you need support in setup of some program or want it to be installed system-wide, please contact the administrators.","title":"FAQ"},{"location":"faq/#how-to-run-jupyter-notebooks-remotely-on-lsscb","text":"Assuming jupyter is running on cn000 (port 8888 ) and that the connection is established with sshuttle (see Getting started section for more details) it is possible setup the tunnel as follows: ssh -NL 8888 :localhost:8888 your_username@cn000.sih-59.internal Afterwards you should be able to see the running Jupyter instance via browser at the URL: http://localhost:8888","title":"How to run Jupyter notebooks remotely on LSSCB?"},{"location":"faq/#how-to-install-python-packages-on-lsscb","text":"python3 (3.10) as well as python2 (2.7.18) along with the recent pip and venv are installed system-wide on each node of LSSCB. You can simply install packages either with pip install --user or use venv or virtualenv (this will allow to handle multiple projects with possibly conflicting dependencies). Finally, if you need newer versions of python or want to handle complicated dependencies you can install a local version of anaconda in your $HOME directory.","title":"How to install python packages on LSSCB?"},{"location":"faq/#i-want-to-use-program-xxx-on-lsscb-can-i-install-it-on-my-own","text":"You can install any software you like in your $HOME directory (as long as you have a valid license to use it). If you need support in setup of some program or want it to be installed system-wide, please contact the administrators.","title":"I want to use program XXX on LSSCB, can I install it on my own?"},{"location":"first_steps/","text":"Setting up an account In order to create your an account on the LSSCB cluster please contact Silvio or Bartosz via e-mail. After the account is approved you will receive credentials via e-mail from the it @ cent.uw.edu.pl address. The obtained password will allow you to login to the entry node (jumphost-59) at sih-59.cent.uw.edu.pl and from there to login to the LSSCB cluster front-node (ssh lsscb) and use the compute nodes cn00[0-2] . Please familiarize yourself with the general rules of cluster usage before proceeding further - LINK Warning Important: in case of lost password or other technical difficulties related to the entry node (not cluster front node or compute nodes) please reach out to the IT department at CeNT UW - address: it @ cent.uw.edu.pl . Include the [sih-59] prefix in the message title and add cluster administrators @Maciek and @Pawel in CC. Information Please note that password changes on each of the compute nodes and the entry node are synced. It is advised to change your initially obtained password after first login. Connecting via SSH Connections to the LSSCB cluster are handled via SSH protocol. See the figure below for a brief introduction of the network organization: In order to login to the entry node you can issue the following command: ssh your_username@sih-59.cent.uw.edu.pl This will bring you to the entry node (jumphost-59) , afterwards you can connect to the LSSCB (10.10.59.30) front node and submit tasks to the compute nodes ( click here for a complete list of available resources), for example: ssh your_username@10.10.59.30 In order to simplify file copying and every day work the suggested way of connecting to the LSSCB cluster is to use sshuttle . This allows to bypass the login node and work almost the same way as being connected via VPN to the local network. Example Assuming sshuttle was installed according to the guide you can connect as follows: sshuttle --dns -NHr your_username@sih-59.cent.uw.edu.pl 10 .10.59.1/24 Once connection is established you can directly login to the LSSCB front node or compute nodes using: ssh your_username@LSSCB.sih-59.internal Information Additionally, depending on your computer and network settings, you may have to connect to LSSCB nodes once without sshuttle so that SSH connections are properly configured. To avoid putting password during each login you can set up authorization via a certificate - additional information is available here Work environment Each user has access a personal directory and can use the shared workspace: /home/users/your_username /workspace Warning These folders are shared between all nodes of the LSSCB cluster as well as the jumphost-59. Transferring files The recommended options to send or fetch files from LSSCB cluster are either scp or rsync . Next steps Once the basics are set up you should be able to start running calculations. Follow the next chapter for more details.","title":"Getting started"},{"location":"first_steps/#setting-up-an-account","text":"In order to create your an account on the LSSCB cluster please contact Silvio or Bartosz via e-mail. After the account is approved you will receive credentials via e-mail from the it @ cent.uw.edu.pl address. The obtained password will allow you to login to the entry node (jumphost-59) at sih-59.cent.uw.edu.pl and from there to login to the LSSCB cluster front-node (ssh lsscb) and use the compute nodes cn00[0-2] . Please familiarize yourself with the general rules of cluster usage before proceeding further - LINK Warning Important: in case of lost password or other technical difficulties related to the entry node (not cluster front node or compute nodes) please reach out to the IT department at CeNT UW - address: it @ cent.uw.edu.pl . Include the [sih-59] prefix in the message title and add cluster administrators @Maciek and @Pawel in CC. Information Please note that password changes on each of the compute nodes and the entry node are synced. It is advised to change your initially obtained password after first login.","title":"Setting up an account"},{"location":"first_steps/#connecting-via-ssh","text":"Connections to the LSSCB cluster are handled via SSH protocol. See the figure below for a brief introduction of the network organization: In order to login to the entry node you can issue the following command: ssh your_username@sih-59.cent.uw.edu.pl This will bring you to the entry node (jumphost-59) , afterwards you can connect to the LSSCB (10.10.59.30) front node and submit tasks to the compute nodes ( click here for a complete list of available resources), for example: ssh your_username@10.10.59.30 In order to simplify file copying and every day work the suggested way of connecting to the LSSCB cluster is to use sshuttle . This allows to bypass the login node and work almost the same way as being connected via VPN to the local network. Example Assuming sshuttle was installed according to the guide you can connect as follows: sshuttle --dns -NHr your_username@sih-59.cent.uw.edu.pl 10 .10.59.1/24 Once connection is established you can directly login to the LSSCB front node or compute nodes using: ssh your_username@LSSCB.sih-59.internal Information Additionally, depending on your computer and network settings, you may have to connect to LSSCB nodes once without sshuttle so that SSH connections are properly configured. To avoid putting password during each login you can set up authorization via a certificate - additional information is available here","title":"Connecting via SSH"},{"location":"first_steps/#work-environment","text":"Each user has access a personal directory and can use the shared workspace: /home/users/your_username /workspace Warning These folders are shared between all nodes of the LSSCB cluster as well as the jumphost-59.","title":"Work environment"},{"location":"first_steps/#transferring-files","text":"The recommended options to send or fetch files from LSSCB cluster are either scp or rsync .","title":"Transferring files"},{"location":"first_steps/#next-steps","text":"Once the basics are set up you should be able to start running calculations. Follow the next chapter for more details.","title":"Next steps"},{"location":"resources/","text":"The New LSSCB cluster currently has in total 156 physical CPUs, 384 GB of RAM and around 110 TB of shared storage space. All Nodes including the Front Node (LSSCB) run Ubuntu 22.04.2 LTS Linux Characteristics of available compute nodes are summarized below: hostname Cores CPU RAM local free space (/tmp) year cn000 36 2x Intel(R) Xeon(R) E5-2697 v 4 @ 2.30GHz 128GB ~ 0.7 TB, SSD - cn001 36 2x Intel(R) Xeon(R) E5-2697 v 4 @ 2.30GHz 128GB ~ 0.7 TB, SSD - cn002 36 2x Intel(R) Xeon(R) E5-2697 v 4 @ 2.30GHz 128GB ~ 3.4 TB, SATA - cn003 36 2x Intel(R) Xeon(R) E5-2697 v 4 @ 2.30GHz 128GB ~ 3.4 TB, SATA - cn010 52 2x Intel(R) Xeon(R) Gold 6230R @ 2.10GHz 128GB ~ 3.1 TB, SATA 2023 cn011 52 2x Intel(R) Xeon(R) Gold 6230R @ 2.10GHz 128GB ~ 3.1 TB, SATA 2023 cn012 52 2x Intel(R) Xeon(R) Gold 6230R @ 2.10GHz 128GB ~ 3.1 TB, SATA 2023","title":"Available resources"},{"location":"rules/","text":"Currently available only in Polish. English version coming soon. Poni\u017csze zasady dotycz\u0105 has\u0142a do w\u0119z\u0142a dost\u0119powego sih-59.cent.uw.edu.pl . U\u017cytkownik jest zobowi\u0105zany do ochrony swojego has\u0142a. Has\u0142o ma charakter poufny. Zabronione jest udost\u0119pnianie ujawnianie has\u0142a w jakikolwiek spos\u00f3b (np. przekazanie innej osobie, zapisywanie w formie nie szyfrowanej, pozostawianie na widoku). W przypadku podejrzenia ujawnienia has\u0142a u\u017cytkownik zobowi\u0105zany jest do jego natychmiastowej zmiany oraz przekazania informacji o zdarzeniu do administrator\u00f3w klastra oraz Dzia\u0142u IT CeNT UW. U\u017cytkownik ponosi odpowiedzialno\u015b\u0107 za wszelkie czynno\u015bci wykonane za po\u015brednictwem konta do niego przypisanego zabezpieczonego jego has\u0142em. Tworz\u0105c has\u0142o nale\u017cy unika\u0107: przyjmowania regularnych schemat\u00f3w w tworzeniu hase\u0142, wybierania sekwencji b\u0105d\u017a powt\u00f3rze\u0144 znak\u00f3w \u0142atwych do podejrzenia, korzystania z nazwy w\u0142asnej u\u017cytkownika, wa\u017cnych dla u\u017cytkownika dat, imion, numer\u00f3w telefon\u00f3w kom\u00f3rkowych, numer\u00f3w rejestracyjnych aut. Zasady tworzenia hase\u0142: Has\u0142o powinno mie\u0107 co najmniej 10 znak\u00f3w (zalecane minimum 15), has\u0142o powinno zawiera\u0107 wielkie i ma\u0142e litery, znaki specjalne, cyfry (0 - 9). Zabrania si\u0119: podawania has\u0142a w wiadomo\u015bciach e-mail, b\u0105d\u017a te\u017c w odpowiedzi na \u017c\u0105danie, kt\u00f3re zosta\u0142o przes\u0142ane poczt\u0105 e-mail, wpisywania hase\u0142 do komputer\u00f3w powstaj\u0105cych w u\u017cytku publicznym np. komputery w kafejkach internetowych, hotelach, bibliotekach itd, zapisywania hase\u0142 w plikach niezaszyfrowanych. U\u017cytkownik zobligowany jest do zmiany has\u0142a nadanego przez administratora przy pierwszej pr\u00f3bie logowania. Zmiana has\u0142a dost\u0119powego do w\u0119z\u0142a dost\u0119powego wymuszana jest co 90 dni. Informacje o utracie has\u0142a, konieczno\u015bci jego zresetowania oraz inne problemy techniczne powinny by\u0107 zg\u0142aszane na adres dzia\u0142u IT CeNT UW - it @ cent.uw.edu.pl do\u0142\u0105czaj\u0105c w CC administrator\u00f3w klasta. Temat wiadomo\u015bci powinien rozpoczyna\u0107 si\u0119 od prefiksu [sih-59] .","title":"General rules"},{"location":"running_jobs/","text":"Introduction Jobs (both batch and interactive sessions) on LSSCB should be run through slurm resource manager. For the quick overview of slurm you can refer to the video: link Information Slurm details: There is currently 1 partition all . The time of execution is limited to 40 days on each partition. Example To get the information on the currently running jobs run squeue : ~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 87719 all interact username R 11 -18:07:21 1 cn001 To get the information on the slurm partitions and their details run sinfo : ~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST all* up 40 -00:00:0 3 idle cn [ 000 -002 ] Interactive sessions LSSCB can be used for interactive work with data, e.g. performing ad-hoc analyses and visualizations with python and jupyter-notebooks. You can either start an interactive session using srun or allocate resources using the salloc utility and then ssh into that host and work there. You can tweak your allocation depending on work needs, see the following table for details and examples. Both commands have a similar set of options: Argument Description -n Number of cores used allocated for the job ( default = 1, max = 36 ) --gres Number of GPUs allocated for the job (_default = None, --gres=gpu, --gres=gpu:2) --mem Amount of memory (in GBs) per allocated core allocated for the job ( default = 1, max = 60 ) -w Specify host or hosts to get your resources (i.e. cn001) Example To login interactively to cn002 with 8 cores and a total of 12 GB of memory : srun -n 8 -w cn002 --mem = 12 --pty bash To obtain an allocation on cn002 with 8 cores and a total of 12 GB of memory : salloc -n 8 -w cn002 --mem = 12 Important! Please remember to quit your interactive allocation when you're done with your work. You can do it by simply typing exit (or CTRL+D). Batch jobs Longer, resource demanding jobs typically should be scheduled in SLURM batch mode. Below you can find the example of the SLURM batch script that you can use to schedule a job: Example Suppose the following job.sh batch file: #!/bin/bash #SBATCH -p all # all partition #SBATCH -n 8 # 8 cores #SBATCH --mem=30GB # 30 GB of RAM #SBATCH -J job_name # name of your job your_program -i input_file -o output_path You can submit the specified job via sbatch command: ~$ sbatch job.sh Submitted batch job 1234","title":"Running calculations"},{"location":"running_jobs/#introduction","text":"Jobs (both batch and interactive sessions) on LSSCB should be run through slurm resource manager. For the quick overview of slurm you can refer to the video: link Information Slurm details: There is currently 1 partition all . The time of execution is limited to 40 days on each partition. Example To get the information on the currently running jobs run squeue : ~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 87719 all interact username R 11 -18:07:21 1 cn001 To get the information on the slurm partitions and their details run sinfo : ~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST all* up 40 -00:00:0 3 idle cn [ 000 -002 ]","title":"Introduction"},{"location":"running_jobs/#interactive-sessions","text":"LSSCB can be used for interactive work with data, e.g. performing ad-hoc analyses and visualizations with python and jupyter-notebooks. You can either start an interactive session using srun or allocate resources using the salloc utility and then ssh into that host and work there. You can tweak your allocation depending on work needs, see the following table for details and examples. Both commands have a similar set of options: Argument Description -n Number of cores used allocated for the job ( default = 1, max = 36 ) --gres Number of GPUs allocated for the job (_default = None, --gres=gpu, --gres=gpu:2) --mem Amount of memory (in GBs) per allocated core allocated for the job ( default = 1, max = 60 ) -w Specify host or hosts to get your resources (i.e. cn001) Example To login interactively to cn002 with 8 cores and a total of 12 GB of memory : srun -n 8 -w cn002 --mem = 12 --pty bash To obtain an allocation on cn002 with 8 cores and a total of 12 GB of memory : salloc -n 8 -w cn002 --mem = 12 Important! Please remember to quit your interactive allocation when you're done with your work. You can do it by simply typing exit (or CTRL+D).","title":"Interactive sessions"},{"location":"running_jobs/#batch-jobs","text":"Longer, resource demanding jobs typically should be scheduled in SLURM batch mode. Below you can find the example of the SLURM batch script that you can use to schedule a job: Example Suppose the following job.sh batch file: #!/bin/bash #SBATCH -p all # all partition #SBATCH -n 8 # 8 cores #SBATCH --mem=30GB # 30 GB of RAM #SBATCH -J job_name # name of your job your_program -i input_file -o output_path You can submit the specified job via sbatch command: ~$ sbatch job.sh Submitted batch job 1234","title":"Batch jobs"},{"location":"software/","text":"Name Version Hosts Location Maintainer autodock vina 1.1.2 all /opt/autodock_vina_1_1_2_linux_x86/* @Pawel autodock 4.2.6 all /opt/autodock_x86_64Linux2/* @Pawel dftbplus 19.1 all /opt/dftbplus-19.1.x86_64-linux/* @Pawel dftbplus 22.1 all /opt/dftbplus-22.1.x86_64-linux/* @Pawel Gaussian 16 all /opt/g16/* @Pawel GaussView 6.1.1 all /opt/gv/* @Pawel gromacs 2023.2 all /opt/gromacs-2023.2/* @Pawel intel oneAPI Base Toolkit 2023.2 all /opt/intel/* @Pawel intel oneAPI HPC Toolkit 2023.2 all /opt/intel/* @Pawel jdftx 1.7.0 all /opt/jdftx/* @Pawel julia 1.9.2 all /opt/julia-1.9.2/* @Pawel NAMD 2.14 all /opt/NAMD_2.14_Linux-x86_64-multicore/* @Pawel orca 5.0.4 all /opt/orca_5_0_4_linux_x86-64_shared_openmpi411/* @Pawel perl 5.34 all /usr @Pawel psi4conda 4.13 all /opt/psi4conda/* @Pawel python 2.7 and 3.10 all /usr @Pawel qchem QCHEM_21APRIL_2021 all /opt/qchem/* @Pawel schrodinger 2021-2 all /opt/schrodinger2021-2/* @Pawel schrodinger 2022-4 all /opt/schrodinger2022-4/* @Pawel vasp 6.4.2 all /opt/vasp.6.4.2/* @Pawel xtb 6.2.2 all /opt/xtb_6.2.2/* @Pawel xtb 6.4.1 all /opt/xtb-6.4.1/* @Pawel xtb 6.5.1 all /opt/xtb-6.5.1/* @Pawel","title":"Available software"}]}